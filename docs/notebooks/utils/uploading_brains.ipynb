{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brainlit.utils.ngl_pipeline import NeuroglancerSession\n",
    "from brainlit.utils import upload_to_neuroglancer as upload\n",
    "from brainlit.utils import upload_skeleton\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "%gui qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = str(Path().resolve().parents[2] / \"tests\" / \"data_octree\")\n",
    "dest_dir = str(Path() / \"upload\")\n",
    "\n",
    "num_res = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Brain Images in the Octree Format\n",
    "## This is a script for uploading entire brain volumes, or uploading specific resolutions onto AWS or a local directory. \n",
    "## Data must be tif files arranged in folders where the highest level corresponds to a single, low res image\n",
    "### Files should be arranged as octree with 1-8 indicating volume octant, Binary paths are used to stitch together images according to resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "got files and binary representations of paths.\ngot dimensions of volume\nLow res files: ['/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/default.0.tif']\n\nHigh res files: ['/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/7/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/6/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/1/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/8/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/4/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/3/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/2/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/5/default.0.tif']\n---\nSingle image binary: [[]]\n\nMultiple image binaries: [['110'], ['101'], ['000'], ['111'], ['011'], ['010'], ['001'], ['100']]\n"
    }
   ],
   "source": [
    "files, bin_paths, vox_size, tiff_dims = upload.get_volume_info(data_dir, num_res, channel = 0)\n",
    "print(\"Low res files: \" + str(files[0]))\n",
    "print(\"\\nHigh res files: \" + str(files[1]))\n",
    "print(\"---\")\n",
    "print(\"Single image binary: \" + str(bin_paths[0]))\n",
    "print(\"\\nMultiple image binaries: \" + str(bin_paths[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloudvolume image layers are created with the number of resolutions in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of volumes: 2\nmips: 1 and 0\nVolumes info: {'data_type': 'uint16', 'num_channels': 1, 'scales': [{'chunk_sizes': [[66, 50, 52]], 'encoding': 'raw', 'key': '6173_6173_6173', 'resolution': [6173, 6173, 6173], 'size': [1056, 800, 416], 'voxel_offset': [0, 0, 0]}, {'chunk_sizes': [[66, 50, 52]], 'encoding': 'raw', 'key': '12346_12346_12346', 'resolution': [12346, 12346, 12346], 'size': [528, 400, 208], 'voxel_offset': [0, 0, 0]}], 'type': 'image'}\n---\nHigh res volume info: {'chunk_sizes': [[66, 50, 52]], 'encoding': 'raw', 'key': '6173_6173_6173', 'resolution': [6173, 6173, 6173], 'size': [1056, 800, 416], 'voxel_offset': [0, 0, 0]}\n\nLow res volume info: {'chunk_sizes': [[66, 50, 52]], 'encoding': 'raw', 'key': '12346_12346_12346', 'resolution': [12346, 12346, 12346], 'size': [528, 400, 208], 'voxel_offset': [0, 0, 0]}\n"
    }
   ],
   "source": [
    "vols = upload.create_image_layer(\"file://\" + dest_dir,tiff_dims, vox_size, num_res)\n",
    "print(\"Number of volumes: \" + str(len(vols)))\n",
    "print(\"mips: \" + str(vols[0].mip) + ' and ' + str(vols[1].mip))\n",
    "print(\"Volumes info: \" + str(vols[0].info))\n",
    "print(\"---\")\n",
    "print(\"High res volume info: \" + str(vols[0].info['scales'][0]))\n",
    "print(\"\\nLow res volume info: \" + str(vols[1].info['scales'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading can be done with either Joblib parallel or non-parrallel sequential if the cpu power isn't there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "u1=upload.upload_chunks(vols[0], files[0], bin_paths[0], parallel=False) # Low res\n",
    "u2=upload.upload_chunks(vols[1], files[1], bin_paths[1], parallel=False) # High res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize your data with NeuroglancerSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading:   0%|          | 0/256 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/256 [00:00<?, ?it/s]\nDownloading: 424it [00:00, 641.30it/s]\nDownloading: 486it [00:00, 712.58it/s]\nDownloading: 358it [00:01, 305.47it/s]\nDownloading: 448it [00:01, 446.07it/s]\nDownloading: 504it [00:01, 266.37it/s]\nDownloading: 488it [00:00, 596.86it/s]\n"
    }
   ],
   "source": [
    "mip = 0 # this can be either 0 or 1\n",
    "tiff_dims = [528*(2-mip),400*(2-mip),208*(2-mip)]\n",
    "ngl_sess = NeuroglancerSession(mip = mip, url = \"file://\" + dest_dir)\n",
    "from cloudvolume import Bbox\n",
    "img = ngl_sess.pull_bounds_img(Bbox((0,0,0), tiff_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with napari.gui_qt():\n",
    "    ngl_sess.napari_viewer(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also load SWC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading Neuron traces in .swc format locally\n",
    "swc_dir = str(Path().resolve().parents[2] / \"tests\" / \"data_swcs\")\n",
    "dest_dir_skel = str(Path() / \"upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "converting swcs to neuroglancer format...: 100%|██████████| 1/1 [00:00<00:00, 79.06it/s]\nUploading: 100%|██████████| 1/1 [00:00<00:00, 155.74it/s]\nUploading: 100%|██████████| 1/1 [00:00<00:00, 143.76it/s]\n"
    }
   ],
   "source": [
    "data_dir = str(Path().resolve().parents[2] / \"tests\" / \"data_octree\")\n",
    "(origin, vox_size, tiff_dims) = upload_skeleton.get_volume_info(data_dir, num_res)\n",
    "vol = upload_skeleton.create_skeleton_layer(\n",
    "    \"file://\"+dest_dir_skel, vox_size, tiff_dims, num_res\n",
    ")\n",
    "skeletons, segids = upload_skeleton.create_skel_segids(swc_dir, origin)\n",
    "for skel in skeletons:\n",
    "    vol.skeleton.upload(skel)\n",
    "\n",
    "vol.skeleton.upload(skel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngl_sess = NeuroglancerSession(mip = 1, url = \"file://\" + dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ngl_sess.pull_voxel(2, 6, nx=10, ny=10, nz=10) # currently mip mismatch and scale mismatch\n",
    "from cloudvolume import Bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_local_volume_around_vertex(ngl_sess, SEGID=2, VID=6, radius=10):\n",
    "    skel = ngl_sess.cv.skeleton.get(SEGID)\n",
    "    vertex = skel.vertices[VID]\n",
    "    # below necessary to compensate for MIP differences\n",
    "    # because the .swc file assumes 7 levels of resolution, \n",
    "    # but our test data only has the first 2\n",
    "    scales = np.multiply(ngl_sess.cv.scales[1][\"resolution\"],2**3) # 3 for mip 1, 4 for mip 0\n",
    "    voxel = np.round(np.divide(vertex, scales)).astype(int)\n",
    "    bounds = Bbox(voxel, voxel)\n",
    "    seed = bounds.to_list()\n",
    "    shape = [radius, radius, radius]\n",
    "    bounds = Bbox(np.subtract(seed[:3], shape), np.add(np.add(seed[3:], shape), 1))\n",
    "    img = ngl_sess.cv.download(bounds)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: 100%|██████████| 1/1 [00:00<00:00, 319.06it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "OutOfBoundsError",
     "evalue": "Bbox([0, 0, 0],[1056, 800, 416], dtype=int32) did not fully contain the specified bounding box Bbox([4944, 550, 1724],[4947, 553, 1727], dtype=int32).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfBoundsError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-15ad7abeedd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# img = get_local_volume_around_vertex(ngl_sess, VID=50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngl_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull_voxel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 9891, 1102, 3449\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spring20/ndd/brainlit/brainlit/utils/ngl_pipeline.py\u001b[0m in \u001b[0;36mpull_voxel\u001b[0;34m(self, seg_id, v_id, nx, ny, nz)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mvox_in_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoxel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvox_in_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/brainlit/lib/python3.7/site-packages/cloudvolume/frontends/precomputed.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, bbox, mip, parallel, segids, preserve_zeros)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m       \u001b[0mbounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m       \u001b[0mautocrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/brainlit/lib/python3.7/site-packages/cloudvolume/lib.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, obj, context, bounded, autocrop)\u001b[0m\n\u001b[1;32m    324\u001b[0m         raise OutOfBoundsError(\n\u001b[1;32m    325\u001b[0m           \"{} did not fully contain the specified bounding box {}.\".format(\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         ))\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfBoundsError\u001b[0m: Bbox([0, 0, 0],[1056, 800, 416], dtype=int32) did not fully contain the specified bounding box Bbox([4944, 550, 1724],[4947, 553, 1727], dtype=int32)."
     ]
    }
   ],
   "source": [
    "# img = get_local_volume_around_vertex(ngl_sess, VID=50)\n",
    "img = ngl_sess.pull_voxel(2, 50)\n",
    "# 9891, 1102, 3449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/1 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/32 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/32 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/32 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/32 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/32 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/32 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/32 [00:00<?, ?it/s]\nDownloading:   0%|          | 0/32 [00:00<?, ?it/s]\n"
    }
   ],
   "source": [
    "img2 = ngl_sess.pull_bounds_img(Bbox(\n",
    "    (0,0,0), (int(9891/2**5), int(1102/2**5), int(3449/2**5))\n",
    "    ))\n",
    "img2 = ngl_sess.pull_bounds_img(Bbox(\n",
    "    (0,0,0), (528, 400, 208)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with napari.gui_qt():\n",
    "    ngl_sess.napari_viewer(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove TF  \n",
    "allow stuff to run on Windows  \n",
    "add more tests (actual unit tests!)\n",
    "\n",
    " - zoomed in retrieval of image\n",
    " - whole brain in napari\n",
    "\n",
    "overlay SWC visualization (verify swc location, test!)  \n",
    "(this is where it should be == this is where it is)\n",
    "\n",
    "Why is the subvolume so bright?  \n",
    "Janky with incomplete octree\n",
    " - swc file assumes 7 levels of resolution, we only have 2\n",
    " - interesting bug maybe? VID=100, radius=50\n",
    "Use floats (possible cloudvolume PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
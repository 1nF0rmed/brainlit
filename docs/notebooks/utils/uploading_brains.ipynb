{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brainlit.utils.ngl_pipeline import NeuroglancerSession\n",
    "from brainlit.utils import upload_to_neuroglancer as upload\n",
    "from brainlit.utils import upload_skeleton\n",
    "from cloudvolume import Bbox\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import napari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Brain Images from data in the Octree format.\n",
    "This notebook demonstrates uploading the 2 lowest-resolution brain volumes, as well as a `.swc` segment file.\n",
    "The upload destination could easily be set to a url of a cloud data server such as s3.\n",
    "\n",
    "## 1) Define variables.\n",
    " - `p` is the prefix string. Filepaths take `file://`, while it may also be `s3://` or `gc://`\n",
    " - `source` and `source_segments` are the root directories of the octree-formatted data and swc files.\n",
    " - `dest` and `dest_segments` are the destinations for the uploads (in this case, a filepath)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"file://\"\n",
    "source = str(Path().resolve().parents[2] / \"tests\" / \"data_octree\")\n",
    "source_segments = str(Path().resolve().parents[2] / \"tests\" / \"data_swcs\")\n",
    "dest = str(Path() / \"upload\")\n",
    "dest_segments = str(Path() / \"upload_segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Gather information from the octree data.\n",
    "Data on filepaths, binarized paths, voxel size, and image dimensions are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got files and binary representations of paths.\n",
      "got dimensions of volume\n",
      "Low res files: ['/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/default.0.tif']\n",
      "\n",
      "High res files: ['/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/7/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/6/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/1/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/8/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/4/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/3/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/2/default.0.tif', '/Users/bijanvarjavand/Documents/spring20/ndd/brainlit/tests/data_octree/5/default.0.tif']\n",
      "---\n",
      "Single image binary: [[]]\n",
      "\n",
      "Multiple image binaries: [['110'], ['101'], ['000'], ['111'], ['011'], ['010'], ['001'], ['100']]\n"
     ]
    }
   ],
   "source": [
    "files, bin_paths, vox_size, tiff_dims = upload.get_volume_info(source, 2, channel=0)\n",
    "print(\"Low res files: \" + str(files[0]))\n",
    "print(\"\\nHigh res files: \" + str(files[1]))\n",
    "print(\"---\")\n",
    "print(\"Single image binary: \" + str(bin_paths[0]))\n",
    "print(\"\\nMultiple image binaries: \" + str(bin_paths[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Genrate an `info` file and generate a Cloudvolume `volume` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of volumes: 2\n",
      "mips: 1 and 0\n",
      "Volumes info: {'data_type': 'uint16', 'num_channels': 1, 'scales': [{'chunk_sizes': [[66, 50, 52]], 'encoding': 'raw', 'key': '9560_9741_31628', 'resolution': [9560, 9741, 31628], 'size': [1056, 800, 416], 'voxel_offset': [0, 0, 0]}, {'chunk_sizes': [[66, 50, 52]], 'encoding': 'raw', 'key': '19120_19482_63256', 'resolution': [19120, 19482, 63256], 'size': [528, 400, 208], 'voxel_offset': [0, 0, 0]}], 'type': 'image'}\n",
      "---\n",
      "High res volume info: {'chunk_sizes': [[66, 50, 52]], 'encoding': 'raw', 'key': '9560_9741_31628', 'resolution': [9560, 9741, 31628], 'size': [1056, 800, 416], 'voxel_offset': [0, 0, 0]}\n",
      "\n",
      "Low res volume info: {'chunk_sizes': [[66, 50, 52]], 'encoding': 'raw', 'key': '19120_19482_63256', 'resolution': [19120, 19482, 63256], 'size': [528, 400, 208], 'voxel_offset': [0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "vols = upload.create_image_layer(p+dest, tiff_dims, vox_size, 2)\n",
    "print(\"Number of volumes: \" + str(len(vols)))\n",
    "print(\"mips: \" + str(vols[0].mip) + ' and ' + str(vols[1].mip))\n",
    "print(\"Volumes info: \" + str(vols[0].info))\n",
    "print(\"---\")\n",
    "print(\"High res volume info: \" + str(vols[0].info['scales'][0]))\n",
    "print(\"\\nLow res volume info: \" + str(vols[1].info['scales'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Upload the image data.\n",
    "This can be done with either Joblib parallel or non-parrallel sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "u1=upload.upload_chunks(vols[0], files[0], bin_paths[0], parallel=False) # Low res\n",
    "u2=upload.upload_chunks(vols[1], files[1], bin_paths[1], parallel=False) # High res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 2-4 are done for the segment data below.\n",
    "Note that we are using a different package and slightly different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting swcs to neuroglancer format...: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2 Get info from source directory\n",
    "origin, _, _ = upload_skeleton.get_volume_info(source, 2) \n",
    "# 3 Create segment info file and volume object\n",
    "vol = upload_skeleton.create_skeleton_layer(p+source_segments, vox_size, tiff_dims, 2)\n",
    "# 4 upload the data\n",
    "skeletons, segids = upload_skeleton.create_skel_segids(p+dest_segments, origin)\n",
    "for skel in skeletons:\n",
    "    vol.skeleton.upload(skel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize your data with NeuroglancerSession and napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 256/256 [00:00<00:00, 300.11it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'graph_to_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cf2663104f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngl_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull_bounds_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mG_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngl_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_segments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_to_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'graph_to_paths' is not defined"
     ]
    }
   ],
   "source": [
    "ngl_sess = NeuroglancerSession(mip=1, url=p+dest, url_segments=p+dest_segments)\n",
    "bounds = Bbox((0,0,0), tiff_dims)\n",
    "img = ngl_sess.pull_bounds_img(bounds)\n",
    "G_sub = ngl_sess.get_segments(2, bounds)\n",
    "paths = graph_to_paths(G_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with napari.gui_qt():\n",
    "    viewer = napari.Viewer(ndisplay=3)\n",
    "    viewer.add_image(img)\n",
    "    viewer.add_shapes(data=paths, shape_type='path', edge_width=0.1, edge_color='blue', opacity=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
